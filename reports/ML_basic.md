# 머신러닝(ML)

## 종류

### 지도 학습(Supervised Learning)

- 정답(레이블)이 있는 데이터로 학습
- 데이터 예측
- Ex)이미지 분류(강아지 vs 고양이) , 스팸 메일 필터링, 감정 분석 등

### 비지도 학습(Unsupervised Learning)

- 정답이 없는 데이터로 학습
- 데이터의 숨겨진 구조/특징 발견
- Ex)고객 세분화(군집화), 연관 규칙 학습(장바구니 분석), 차원 축소(이미지 압축) 등

### 강화 학습(Reinforcement Learning)

- 보상 시스템으로 학습
- 의사결정을 위한 최적의 액션 선택
- Ex)게임AI(알파고, 체스 등), 로봇 제어(걷기, 물건집기 등), 자율주행 자동차 등

## 지도 학습 알고리즘

### 회귀: 연속적인 숫자를 예측하는 것

- 연속적인 값을 예측하는 데 사용(Ex: 주택 가격, 온도, 판매량 등).
- 대표적인 예로 선형 회귀(Linear Regression), 다중 회귀(Multiple Regression), 로지스틱 회귀(Logistic Regression) 등이 있다.

### 분류: 입력 데이터를 미리 정의된 여러 개의 클래스 중 하나로 예측하는 것

- 범주형 값을 예측하는데 사용(Ex: 스팸 메일 필터링, 날씨 예측 등).
- 대표적인 예로 결정 트리(Decision Trees), 랜덤 포레스트(Random Forests), 서포트 벡터 머신(SVM), 신경망(Neural Networks) 등이 있다.

# 지도 학습 모델

## 회귀(Regression)

- 서로 연관성을 갖고 일정한 방향으로 증가 혹은 감소하는 추세를 보이는 데이터가 대상
- 추세에 따라서 값을 예측
- 추세선을 함수식으로 구하고, 이 함수식으로 새로운 데이터에 대한 결과를 예측
- 인공지능이 학생이 선택한 과목 수, 학생의 하루 평균 공부한 시간만으로 어떻게 학생의 시험 점수를 예측 하는 것

-공부시간: 입력데이터(독립변수)

-시험 점수: 출력데이터(종속변수)

- 산포도를 보고 공부시간이 증가할 때 시험 점수도 증가하는 것을 알 수 있다. 즉 양의 상관관계가 있음을 알 수 있다.
- 예측에 사용되는 특성(독립변수)과 예측하려는 대상(종속변수)의 관계를 설명할 수 있는 직선은 여러 개가 있으나 그 중에서 실제값과 예측값의 차이가 가장 작은 것이 두 변수의 관계를 가장 잘 설명할 수 있다.

### 회귀분석 적용 분야 예시

- **금 가격(종속변수)**
    - 이자율
    - 물가 상승률
    - 석유 가격
    - 보석용 금에 대한 수요
    - 산업용 금에 대한 수요
    - 상업용 금에 대한 수요
- **주택 가격(종속변수)**
    - 주택의 크기
    - 침실의 수
    - 도로 접근성
    - 주택의 위치
    - 주택의 상태
    - 옵션
- **중고차 가격(종속변수)**
    - 차종
    - 배기량
    - 연식
    - 관리상태
    - 옵션
    - 사고여부

## 선형 회귀(Linear Regression) 알고리즘

### 의미

- 독립 변수 X와 종속 변수 Y의 관계를 표현하는 선형 방정식을 찾고, 찾아낸 방정식을 이용하여 새로운 입력값에 대한 결괏값을 예측하는 모델링 방법

$$
Y = w_0 + w_1 * X
$$

-추세선을 그리는 선형 방정식에서 예측값과 실제값 사이의 차이가 작아지는 $w_0$값(절편)과 $w_1$값(기울기)을 찾는 것이 목표이다(회귀선을 찾는 알고리즘).

- 회귀문제는 데이터가 정답에 얼마나 가깝게 예측하는가, 예측한 값이 정답과 오차가 얼마나 적은지로 인공지능의 성능을 판단한다.

## 분류(classification)

- 레이블이 달린 학습 데이터로 학습한 후에 새로 입력된 데이터가 학습했던 어느 그룹에 속하는 지를 찾아내는 방법
- 남성과 여성, 참과 거짓, 스팸인지 아닌지과 같은 이산적인 값을 예측
- 분류문제는 이미 정답이 주어져 있기 때문에 정답률이 얼마나 되는지로 인공지능의 성능을 판단한다.
- 데이터의 연속성: 연속적인 수치를 갖는 데이터를 예측하는 회귀와 달리 분류는 이산적인 수치를 갖는 데이터로 예측을 해야한다.

## k-Nearest Neighbor(kNN) 알고리즘

- 개념: 가장 가까운 K개의 이웃 데이터를 기반으로 분류하는 알고리즘
- 특징:
    - 단순하고 효율적이다.
    - 새로운 데이터에 대한 학습이 필요없다.
    - k-최근접 이웃 알고리즘이라고도 한다.
- 단점:
    - 큰 데이터 세트에는 계산 비용이 높다.
    - 새로운 데이터의 레이블을 예측할 때 이웃의 레이블의 비율에 따라 예측이 달라질 수 있다.

-k 값은 학습 데이터로 학습을 완료한 뒤에 테스트 데이터를 활용해서 k 값을 작은 것부터 시작해서 큰 값으로 바꾸어가면서 가장 높은 정확도를 나타내는 k 값을 찾아내는 방식으로 찾는다.

## Decision Tree(결정 트리)알고리즘

- 개념: 데이터를 분류하기 위해 결정 규칙을 트리 구조로 나타내는 방법
- 특징:
    - 이해하기 쉽고 해석 가능
    - 다양한 데이터 세트에 적용 할 수 있음
    - 계산 비용이 비교적 적음
- 단점:
    - 과적합(Overfitting)의 위험
    
    -과적합: 모델이 학습 데이터에 과적합되어 새로운 데이터에 잘 작동하지 못하는 것. 
    
    -훈련 데이터 세트로 모델을 학습하고 테스트 데이터 세트로 모델의 성능을 평가했을 때 테스트 데이터 셋에서 모델의 성능이 향상되지 않으면 모델이 과적합 되었다고 판단한다. 
    
    - 데이터의 특성이 많으면 복잡해짐

## Suppot Vector Machine(SVM) 알고리즘

- 개념: 데이터를 분류하기 위해 결정 경계(Decision Boundaty)를 정의하는 모델
- 특징:
    - 성능이 우수
    - 마진(Margin) 최적화를 통한 분류 정확도 향상
- 단점:
    - 계산 비용이 많이 소요
    - 설정이 복잡

-최적의 선(두 클래스의 데이터를 잘 구별하는 결정 경계)을 찾기 위한 알고리즘.

-실제로 분류나 회귀, 이상치 탐지 등의 다양한 머신러닝 문제에 사용되는 알고리즘이다.

## Ex)키에 대한 몸무게 예측(선형 회귀)

```python
import numpy as np
from sklearn import linear_model

regr = linear_model.LinearRegression()  #선형 회귀 모델 객체 생성
x = [[164], [179], [162], [170]]    #키 데이터
y = [53, 63, 55, 59]    #몸무게 데이터
regr.fit(x, y)  #모델 학습

score = regr.score(x, y)
print("The score of this line for the data: ", score)

input_data = [[180], [185]] #예측에 사용할 키 데이터
result = regr.predict(input_data)   #예측 수행
print(result)
```

- 산점도 및 회귀선 추가(맷플롯립)

```python
import numpy as np
from sklearn import linear_model
import matplotlib.pyplot as plt

regr = linear_model.LinearRegression()  #선형 회귀 모델 객체 생성
x = [[164], [179], [162], [170]]    #키 데이터
y = [53, 63, 55, 59]    #몸무게 데이터
regr.fit(x, y)  #모델 학습

plt.scatter(x, y)   #산점도 그리기
y_pred = regr.predict(x)    #예측 수행

plt.plot(x, y_pred, color='blue', linewidth=3)  #회귀선 그리기
plt.show()
```

- 선형 회귀 알고리즘을 활용한 회귀 모델 만들기

-문제 정의와 데이터 수집: 재배 면적과 수확량이 주어졌을 때 선형 회귀 머신러닝 모델을 완성하고, 모델을 평가한다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = np.array([[2.1], [10], [3], [1], [3.5], [5], [8]], dtype=float)    #재배 면적
y = np.array([64.9, 292.6, 85.9, 30.92, 110.5, 163.4, 230.1], dtype=float)  #수확량

LR_model = LinearRegression()
LR_model.fit(x, y)

prd = LR_model.predict([[7], [12]])
print(prd)

plt.scatter(x, y, label='Train Data')
plt.scatter([7, 12], prd, marker='v', label='Predicted Data')
plt.show()
```

1. 데이터 준비
2. 산점도를 통해 데이터의 상관관계 분석
3. 선형 회귀 알고리즘 (선택)
4. 모델 생성
5. 모델 학습
6. 결과 예측
7. 그래프(시각화)
