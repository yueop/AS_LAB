{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPu2puVWsrYBHmJ5fxjUoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yueop/AS_LAB/blob/main/datasets_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAKqZOJJFxC4",
        "outputId": "acde0c3c-ae8a-4f83-c7e3-04a4c7b7a174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "기존 폴더에 저장: /content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')   #구글 드라이브 연결\n",
        "Project_Path = '/content/drive/MyDrive/Colab Notebooks'  #저장할 폴더 경로 설정\n",
        "\n",
        "#저장할 폴더 존재 여부 확인(없으면 새 폴더 생성, 있으면 기존 폴더 사용)\n",
        "if not os.path.exists(Project_Path):\n",
        "    os.makedirs(Project_Path)\n",
        "    print(f\"새 폴더 생성 후 저장 완료: {Project_Path}\")\n",
        "else:\n",
        "    print(f\"기존 폴더에 저장: {Project_Path}\")\n",
        "\n",
        "os.chdir(Project_Path)   #작업 위치 이동(change directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_datasets.py\n",
        "import torch #일반적인 파이토치 기능(텐서 만들기 등)\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split #데이터를 병합하고, 운송하고, 분할하는 기능\n",
        "from torchvision import datasets, transforms #이미지를 모델이 알아볼 수 있게 변형하고, 대중적인 이미지 데이터셋들 가져오는 기능\n",
        "from sklearn.datasets import load_iris #IRIS 데이터셋 로드\n",
        "from sklearn.model_selection import train_test_split #train_dataset과 test_dataset을 랜덤하게 섞어 비율대로 잘라주는 기능\n",
        "from sklearn.preprocessing import StandardScaler #IRIS 정규화 용도\n",
        "\n",
        "class UnifiedDataLoader:\n",
        "    def __init__(self, batch_size=64, aug_config=None):\n",
        "        self.batch_size = batch_size\n",
        "        self.aug_config = aug_config if aug_config else {'use_aug': False}\n",
        "\n",
        "    def get_transform(self, dataset_name, mode):\n",
        "        #IRIS는 변형 X\n",
        "        if dataset_name == 'iris':\n",
        "            return None\n",
        "\n",
        "        #기본 변환(ToTensor, Nomalize)\n",
        "        base_transforms = [\n",
        "            transforms.ToTensor(), #이미지 데이터 텐서로 변환(전처리)\n",
        "            transforms.Normalize((0.5,), (0.5,)) #데이터 정규화(-1.0~1.0 사이의 값으로 변환)\n",
        "            ]\n",
        "\n",
        "        if mode == 'train' and self.aug_config.get('use_aug', False): #학습 모드이면서 증강 설정이 True라면 증강 적용\n",
        "            aug_list = []\n",
        "            #회전\n",
        "            if self.aug_config.get('rotation', 0) > 0:\n",
        "                aug_list.append(transforms.RandomRotation(self.aug_config['rotation']))\n",
        "            #좌우 반전\n",
        "            if self.aug_config.get('flip_prob', 0) > 0:\n",
        "                aug_list.append(transforms.RandomHorizontalFlip(p=self.aug_config['flip_prob']))\n",
        "            #증강 -> 기본 변환 순서로 합치기\n",
        "            return transforms.Compose(aug_list + base_transforms)\n",
        "\n",
        "        else:\n",
        "            #검증 / 테스트 할 때는 기본 변환만 적용\n",
        "            return transforms.Compose(base_transforms)\n",
        "\n",
        "    def get_loader(self, dataset_name, mode='train'):\n",
        "        dataset_name = dataset_name.lower()\n",
        "        if dataset_name == 'iris':\n",
        "            return self.get_iris_loader(mode)\n",
        "        elif dataset_name == 'mnist':\n",
        "            return self.get_mnist_loader(mode)\n",
        "        elif dataset_name == 'fashionmnist':\n",
        "            return self.get_fashionmnist_loader(mode)\n",
        "        else:\n",
        "            print(f'지원하지 않는 데이터셋입니다.: {dataset_name}') #예외 처리\n",
        "            return None\n",
        "\n",
        "    def get_iris_loader(self, mode): #IRIS 데이터셋 로더\n",
        "        iris = load_iris() #사이킷런 라이브러리에서 데이터 로드\n",
        "        X, y = iris.data, iris.target\n",
        "\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42) #데이터 분할\n",
        "\n",
        "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "        scaler = StandardScaler() #IRIS 데이터 정규화\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_val = scaler.transform(X_val)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        if mode == 'train':\n",
        "            X_data, y_data = X_train, y_train\n",
        "            shuffle = True\n",
        "        elif mode == 'val':\n",
        "            X_data, y_data = X_val, y_val\n",
        "            shuffle = False\n",
        "        else: #test\n",
        "            X_data, y_data = X_test, y_test\n",
        "            shuffle = False\n",
        "\n",
        "        #텐서 변환\n",
        "        X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y_data, dtype=torch.long)\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        return DataLoader(dataset, batch_size = self.batch_size, shuffle=shuffle)\n",
        "\n",
        "    def get_image_dataset_split(self, dataset_class, dataset_name, mode):\n",
        "        current_transform = self.get_transform(dataset_name, mode)\n",
        "        if mode == 'test':\n",
        "            dataset = dataset_class(root='./data', train=False, download=True, transform=current_transform)\n",
        "            return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        full_train_dataset = dataset_class(root='./data', train=True, download=True, transform=current_transform) #훈련용 데이터 불러오기\n",
        "\n",
        "        train_size = int(0.8 * len(full_train_dataset)) #학습용 데이터(80%)\n",
        "        val_size = len(full_train_dataset) - train_size #검증용 데이터(20%)\n",
        "\n",
        "        train_dataset, val_dataset = random_split(\n",
        "            full_train_dataset, [train_size, val_size],\n",
        "            generator = torch.Generator().manual_seed(42)\n",
        "        )\n",
        "\n",
        "        if mode == 'train':\n",
        "            return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        else: #valid\n",
        "            return DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def get_mnist_loader(self, mode):\n",
        "        return self.get_image_dataset_split(datasets.MNIST, 'mnist', mode)\n",
        "    def get_fashionmnist_loader(self, mode):\n",
        "        return self.get_image_dataset_split(datasets.FashionMNIST, 'fashionmnist', mode)"
      ],
      "metadata": {
        "id": "-v5Fw1YEVZ3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1dcdb2-6560-4c5d-cf65-7e941faf1c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting my_datasets.py\n"
          ]
        }
      ]
    }
  ]
}